{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1H_t9UbuFoCu-GPAIkrU7NagrNzLdAALy",
      "authorship_tag": "ABX9TyM8LvYmICGWfHXXyxMcXhse",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shireesh-kumar/RVS-UNet/blob/main/TRAIN_RetinalVesselSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMa4-U5XzwAy"
      },
      "outputs": [],
      "source": [
        "#Code inspired from https://github.com/nikhilroxtomar/Retina-Blood-Vessel-Segmentation-in-PyTorch/tree/main\n",
        "#Dataset : FIVES Retinal Vessel Dataset\n",
        "\n",
        "#Importing requrired libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Constants\n",
        "H = 256\n",
        "W = 256\n",
        "batch_size = 8\n",
        "num_epochs = 20\n",
        "lr = 1e-4\n",
        "checkpoint_path = \"files/checkpoint.pth\"\n",
        "\n",
        "# Pre-allocate tensors for inputs and labels -- Memory Optimization\n",
        "device = torch.device('cuda')\n",
        "inputs_preallocated = torch.zeros(batch_size, 3, H, W, device=device,dtype=torch.float32)\n",
        "labels_preallocated = torch.zeros(batch_size, 1, H, W, device=device, dtype=torch.float32)\n",
        "\n",
        "train_x = sorted(glob(\"/content/drive/MyDrive/FIVES A Fundus Image Dataset for AI-based Vessel Segmentation/train/Original/*.png\"))\n",
        "train_y = sorted(glob(\"/content/drive/MyDrive/FIVES A Fundus Image Dataset for AI-based Vessel Segmentation/train/Ground truth/*.png\"))\n"
      ],
      "metadata": {
        "id": "YzbgjAw-vTps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Custom Class\n",
        "\n",
        "class DriveDataset(Dataset):\n",
        "  def __init__(self, images_path, masks_path):\n",
        "    self.images_path = images_path\n",
        "    self.masks_path = masks_path\n",
        "    self.n_samples = len(images_path)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n",
        "    image = cv2.resize(image, (H, W), interpolation=cv2.INTER_AREA)\n",
        "    image = image/255.0 ## ex.(512, 512, 3)\n",
        "    image = np.transpose(image, (2, 0, 1))  ## ex.(3, 512, 512)\n",
        "    image = image.astype(np.float32)\n",
        "    image = torch.from_numpy(image)\n",
        "\n",
        "    mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n",
        "    mask = cv2.resize(mask, (H, W), interpolation=cv2.INTER_NEAREST)\n",
        "    mask = mask/255.0   ## ex.(512, 512)\n",
        "    mask = np.expand_dims(mask, axis=0) ## ex.(1, 512, 512)\n",
        "    mask = mask.astype(np.float32)\n",
        "    mask = torch.from_numpy(mask)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples"
      ],
      "metadata": {
        "id": "xzqJMRtb0VBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss Function\n",
        "class DiceBCELoss(nn.Module):\n",
        "  def __init__(self, weight=None, size_average=True):\n",
        "    super(DiceBCELoss, self).__init__()\n",
        "\n",
        "  def forward(self, inputs, targets, smooth=1):\n",
        "\n",
        "    inputs = torch.sigmoid(inputs)\n",
        "    inputs = inputs.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    intersection = (inputs * targets).sum()\n",
        "    dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "    BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
        "    Dice_BCE = BCE + dice_loss\n",
        "\n",
        "    return Dice_BCE"
      ],
      "metadata": {
        "id": "QVKu5nXu6oyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Utility Functions\n",
        "def seeding(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)"
      ],
      "metadata": {
        "id": "uPPc9MMJ9DaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "  def __init__(self, in_c, out_c):\n",
        "      super().__init__()\n",
        "\n",
        "      self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
        "      self.bn1 = nn.BatchNorm2d(out_c)\n",
        "\n",
        "      self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
        "      self.bn2 = nn.BatchNorm2d(out_c)\n",
        "\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, inputs):\n",
        "      x = self.conv1(inputs)\n",
        "      x = self.bn1(x)\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "      x = self.bn2(x)\n",
        "      x = self.relu(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "class encoder_block(nn.Module):\n",
        "  def __init__(self, in_c, out_c):\n",
        "      super().__init__()\n",
        "\n",
        "      self.conv = conv_block(in_c, out_c)\n",
        "      self.pool = nn.MaxPool2d((2, 2))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "      x = self.conv(inputs)\n",
        "      p = self.pool(x)\n",
        "\n",
        "      return x, p\n",
        "\n",
        "class decoder_block(nn.Module):\n",
        "  def __init__(self, in_c, out_c):\n",
        "      super().__init__()\n",
        "\n",
        "      self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
        "      self.conv = conv_block(out_c+out_c, out_c)\n",
        "\n",
        "  def forward(self, inputs, skip):\n",
        "      x = self.up(inputs)\n",
        "      x = torch.cat([x, skip], axis=1)\n",
        "      x = self.conv(x)\n",
        "      return x\n",
        "\n",
        "class build_unet(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      \"\"\" Encoder \"\"\"\n",
        "      self.e1 = encoder_block(3, 64)\n",
        "      self.e2 = encoder_block(64, 128)\n",
        "      self.e3 = encoder_block(128, 256)\n",
        "      self.e4 = encoder_block(256, 512)\n",
        "\n",
        "      \"\"\" Bottleneck \"\"\"\n",
        "      self.b = conv_block(512, 1024)\n",
        "\n",
        "      \"\"\" Decoder \"\"\"\n",
        "      self.d1 = decoder_block(1024, 512)\n",
        "      self.d2 = decoder_block(512, 256)\n",
        "      self.d3 = decoder_block(256, 128)\n",
        "      self.d4 = decoder_block(128, 64)\n",
        "\n",
        "      \"\"\" Classifier \"\"\"\n",
        "      self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "      \"\"\" Encoder \"\"\"\n",
        "      s1, p1 = self.e1(inputs)\n",
        "      s2, p2 = self.e2(p1)\n",
        "      s3, p3 = self.e3(p2)\n",
        "      s4, p4 = self.e4(p3)\n",
        "\n",
        "      \"\"\" Bottleneck \"\"\"\n",
        "      b = self.b(p4)\n",
        "\n",
        "      \"\"\" Decoder \"\"\"\n",
        "      d1 = self.d1(b, s4)\n",
        "      d2 = self.d2(d1, s3)\n",
        "      d3 = self.d3(d2, s2)\n",
        "      d4 = self.d4(d3, s1)\n",
        "\n",
        "      outputs = self.outputs(d4)\n",
        "\n",
        "      return outputs\n",
        "\n",
        "# Checking the output of the model\n",
        "# x = torch.randn((2, 3, 512, 512))\n",
        "# f = build_unet()\n",
        "# y = f(x)\n",
        "# print(y.shape)"
      ],
      "metadata": {
        "id": "udxxeAgX-sWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "def train(model, loader, optimizer, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "    for x, y in loader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    epoch_loss = epoch_loss/len(loader)\n",
        "    return epoch_loss\n",
        "\n",
        "def evaluate(model, loader, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            y_pred = model(x)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss = epoch_loss/len(loader)\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    seeding(42)\n",
        "    create_dir(\"files\")\n",
        "\n",
        "\n",
        "    train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
        "\n",
        "    def custom_collate_fn(batch):\n",
        "      for i, data_point in enumerate(batch):\n",
        "          inputs_preallocated[i] = data_point[0]\n",
        "          labels_preallocated[i] = data_point[1]\n",
        "      return inputs_preallocated, labels_preallocated\n",
        "\n",
        "    train_dataset = DriveDataset(train_x, train_y)\n",
        "    valid_dataset = DriveDataset(valid_x, valid_y)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        collate_fn=custom_collate_fn\n",
        "    )\n",
        "\n",
        "    valid_loader = DataLoader(\n",
        "        dataset=valid_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        collate_fn=custom_collate_fn\n",
        "    )\n",
        "\n",
        "    model = build_unet()\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
        "    loss_fn = DiceBCELoss()\n",
        "\n",
        "    best_valid_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
        "        valid_loss = evaluate(model, valid_loader, loss_fn, device)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
        "            print(data_str)\n",
        "\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "        data_str_metric = ''\n",
        "        data_str_metric += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
        "        data_str_metric += f'\\t Val. Loss: {valid_loss:.3f}\\n'\n",
        "        print(data_str_metric)"
      ],
      "metadata": {
        "id": "ij0F46Ur_LpZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeedfea1-b2c6-4de8-baa6-a3d11f07972b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid loss improved from inf to 1.1974. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 1.217\n",
            "\t Val. Loss: 1.197\n",
            "\n",
            "Valid loss improved from 1.1974 to 0.9612. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 1.018\n",
            "\t Val. Loss: 0.961\n",
            "\n",
            "Valid loss improved from 0.9612 to 0.8897. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.926\n",
            "\t Val. Loss: 0.890\n",
            "\n",
            "Valid loss improved from 0.8897 to 0.8251. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.852\n",
            "\t Val. Loss: 0.825\n",
            "\n",
            "Valid loss improved from 0.8251 to 0.7688. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.791\n",
            "\t Val. Loss: 0.769\n",
            "\n",
            "Valid loss improved from 0.7688 to 0.7154. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.735\n",
            "\t Val. Loss: 0.715\n",
            "\n",
            "Valid loss improved from 0.7154 to 0.6658. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.687\n",
            "\t Val. Loss: 0.666\n",
            "\n",
            "Valid loss improved from 0.6658 to 0.6311. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.640\n",
            "\t Val. Loss: 0.631\n",
            "\n",
            "Valid loss improved from 0.6311 to 0.5930. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.599\n",
            "\t Val. Loss: 0.593\n",
            "\n",
            "Valid loss improved from 0.5930 to 0.5570. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.561\n",
            "\t Val. Loss: 0.557\n",
            "\n",
            "Valid loss improved from 0.5570 to 0.5245. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.526\n",
            "\t Val. Loss: 0.524\n",
            "\n",
            "Valid loss improved from 0.5245 to 0.4906. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.487\n",
            "\t Val. Loss: 0.491\n",
            "\n",
            "Valid loss improved from 0.4906 to 0.4745. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.456\n",
            "\t Val. Loss: 0.474\n",
            "\n",
            "Valid loss improved from 0.4745 to 0.4412. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.428\n",
            "\t Val. Loss: 0.441\n",
            "\n",
            "Valid loss improved from 0.4412 to 0.4138. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.399\n",
            "\t Val. Loss: 0.414\n",
            "\n",
            "Valid loss improved from 0.4138 to 0.3980. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.374\n",
            "\t Val. Loss: 0.398\n",
            "\n",
            "Valid loss improved from 0.3980 to 0.3737. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.351\n",
            "\t Val. Loss: 0.374\n",
            "\n",
            "Valid loss improved from 0.3737 to 0.3597. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.326\n",
            "\t Val. Loss: 0.360\n",
            "\n",
            "Valid loss improved from 0.3597 to 0.3414. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.307\n",
            "\t Val. Loss: 0.341\n",
            "\n",
            "Valid loss improved from 0.3414 to 0.3296. Saving checkpoint: files/checkpoint.pth\n",
            "\tTrain Loss: 0.286\n",
            "\t Val. Loss: 0.330\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DhoHCUoNCMZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iRbZ4wlEEjAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}